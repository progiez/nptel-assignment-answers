# NPTEL Introduction To Machine Learning Week 06 Assignment Answers

Are you looking for NPTEL Introduction To Machine Learning Week 06 Assignment Answers? This repository will help you find your answers and solutions for Week 06 of the Introduction To Machine Learning course. We provide detailed solutions to help you complete your assignments efficiently.


<figure class="aligncenter size-large is-resized"><img decoding="async" width="1024" height="576" src="https://progiez.com/wp-content/uploads/2024/08/Introduction-To-Machine-Learning-Nptel-Week-6-Assignment-Answer-and-solution-Swayam-Platform-image-1024x576.webp" alt="Introduction to Machine Learning Nptel Week 6 Answers" class="wp-image-13375" title="Introduction to Machine Learning Nptel Week 6 Answers 1" srcset="https://progiez.com/wp-content/uploads/2024/08/Introduction-To-Machine-Learning-Nptel-Week-6-Assignment-Answer-and-solution-Swayam-Platform-image-1024x576.webp 1024w, https://progiez.com/wp-content/uploads/2024/08/Introduction-To-Machine-Learning-Nptel-Week-6-Assignment-Answer-and-solution-Swayam-Platform-image-300x169.webp 300w, https://progiez.com/wp-content/uploads/2024/08/Introduction-To-Machine-Learning-Nptel-Week-6-Assignment-Answer-and-solution-Swayam-Platform-image-768x432.webp 768w, https://progiez.com/wp-content/uploads/2024/08/Introduction-To-Machine-Learning-Nptel-Week-6-Assignment-Answer-and-solution-Swayam-Platform-image.webp 1280w" sizes="(max-width: 1024px) 100vw, 1024px"><figcaption class="wp-element-caption"><strong>Introduction to Machine Learning Nptel Week<span> </span></strong>6<span> </span><strong>Answers</strong></figcaption></figure>

## **Introduction to Machine Learning Nptel Week 6 Answers (July-Dec 2024)**

* * *

**1. Entropy for a 90-10 split between two classes is:**

A) 0.469  
B) 0.195  
C) 0.204  
D) None of the above

**Answer:** A) 0.469

* * *

**2. Consider a dataset with only one attribute (categorical). Suppose, there are 8 unordered values in this attribute, how many possible combinations are needed to find the best split-point for building the decision tree classifier?**

A) 511  
B) 1023  
C) 512  
D) 127

**Answer:** [Click here to view Answers](https://progiez.com/introduction-to-machine-learning-nptel-week-6-answers)

* * *

**3. Having built a decision tree, we are using reduced error pruning to reduce the size of the tree. We select a node to collapse. For this particular node, on the left branch, there are three training data points with the following outputs: 5, 7, 9.6, and for the right branch, there are four training data points with the following outputs: 8.7, 9.8, 10.5, 11. The average value of the outputs of data points denotes the response of a branch. The original responses for data points along the two branches (left & right respectively) were response−left and response−right and the new response after collapsing the node is response−new. What are the values for response−left, response−right, and response−new (numbers in the option are given in the same order)?**

A) 9.6, 11, 10.4  
B) 7.2; 10; 8.8  
C) 5, 10.5, 15  
D) Depends on the tree height

**Answer:** [Click here to view Answers](https://progiez.com/introduction-to-machine-learning-nptel-week-6-answers)

* * *

**4. Which of the following is a good strategy for reducing the variance in a decision tree?**

A) If improvement of taking any split is very small, don’t make a split. (Early Stopping)  
B) Stop splitting a leaf when the number of points is less than a set threshold K.  
C) Stop splitting all leaves in the decision tree when any one leaf has less than a set threshold K points.  
D) None of the Above.

**Answer:** [Click here to view Answers](https://progiez.com/introduction-to-machine-learning-nptel-week-6-answers)

* * *

**These are Introduction to Machine Learning Nptel Week 6 Answers**

* * *

**5. Which of the following statements about multiway splits in decision trees with categorical features is correct?**

A) They always result in deeper trees compared to binary splits  
B) They always provide better interpretability than binary splits  
C) They can lead to overfitting when dealing with high-cardinality categorical features  
D) They are computationally less expensive than binary splits for all categorical features

**Answer:** [Click here to view Answers](https://progiez.com/introduction-to-machine-learning-nptel-week-6-answers)

* * *

**6. Which of the following statements about imputation in data preprocessing is most accurate?**

A) Mean imputation is always the best method for handling missing numerical data  
B) Imputation should always be performed after splitting the data into training and test sets  
C) Missing data is best handled by simply removing all rows with any missing values  
D) Multiple imputation typically produces less biased estimates than single imputation methods

**Answer:** [Click here to view Answers](https://progiez.com/introduction-to-machine-learning-nptel-week-6-answers)

* * *

**7. Consider the following dataset:**

Which among the following split-points for feature2 would give the best split according to the misclassification error?

A) 186.5  
B) 188.6  
C) 189.2  
D) 198.1

**Answer:** [Click here to view Answers](https://progiez.com/introduction-to-machine-learning-nptel-week-6-answers)

* * *

**These are Introduction to Machine Learning Nptel Week 6 Answers**

All weeks of Introduction to Machine Learning: [Click Here](https://progiez.com/nptel-assignment-answers/introduction-to-internet-of-things)

For answers to additional Nptel courses, please refer to this link: [NPTEL Assignment Answers](https://progiez.com/nptel-assignment-answers)
