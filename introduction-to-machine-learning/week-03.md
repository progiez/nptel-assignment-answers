# NPTEL Introduction To Machine Learning Week 03 Assignment Answers

Are you looking for NPTEL Introduction To Machine Learning Week 03 Assignment Answers? This repository will help you find your answers and solutions for Week 03 of the Introduction To Machine Learning course. We provide detailed solutions to help you complete your assignments efficiently.


## _Introduction to Machine Learning Nptel Week 3 Answers (Jan-Apr 2025)_

***

**1) Which of the following statement(s) about decision boundaries and discriminant functions of classifiers is/are true?**

a) In a binary classification problem, all points xx on the decision boundary satisfy δ1(x)=δ2(x)\delta\_1(x) = \delta\_2(x).\
b) In a three-class classification problem, all points on the decision boundary satisfy δ1(x)=δ2(x)=δ3(x)\delta\_1(x) = \delta\_2(x) = \delta\_3(x).\
c) In a three-class classification problem, all points on the decision boundary satisfy at least one of δ1(x)=δ2(x)\delta\_1(x) = \delta\_2(x), δ2(x)=δ3(x)\delta\_2(x) = \delta\_3(x), δ3(x)=δ1(x)\delta\_3(x) = \delta\_1(x).\
d) If xx does not lie on the decision boundary, then all points lying in a sufficiently small neighborhood around xx belong to the same class.

[View Answer](https://my.progiez.com/courses/introduction-to-machine-learning-answers/)

***

**2) You train an LDA classifier on a dataset with 2 classes. The decision boundary is significantly different from the one obtained by logistic regression. What could be the reason?**

a) The underlying data distribution is Gaussian.\
b) The two classes have equal covariance matrices.\
c) The underlying data distribution is not Gaussian.\
d) The two classes have unequal covariance matrices.

[View Answer](https://my.progiez.com/courses/introduction-to-machine-learning-answers/)

***

**3) The following table gives the binary ground truth labels YiY\_i for four input points (not given). We have a logistic regression model with some parameter values that computes the probability P1(Xi)P\_1(X\_i) that the label is 1. Compute the likelihood of observing the data given these model parameters.**

| YiY\_i           | 1   | 0   | 1   | 0   |
| ---------------- | --- | --- | --- | --- |
| P1(Xi)P\_1(X\_i) | 0.8 | 0.5 | 0.2 | 0.9 |

a) 0.072\
b) 0.144\
c) 0.288\
d) 0.002

[View Answer](https://my.progiez.com/courses/introduction-to-machine-learning-answers/)

***

**4) Which of the following statement(s) about logistic regression is/are true?**

a) It learns a model for the probability distribution of the data points in each class.\
b) The output of a linear model is transformed to the range (0,1) by a sigmoid function.\
c) The parameters are learned by minimizing the mean-squared loss.\
d) The parameters are learned by maximizing the log-likelihood.

[View Answer](https://my.progiez.com/courses/introduction-to-machine-learning-answers/)

***

**5) Consider a modified form of logistic regression given below where kk is a positive constant and β0\beta\_0, β1\beta\_1 are parameters:**

![image 26](https://progiez.com/wp-content/uploads/2024/08/image-26.png "Introduction to Machine Learning Nptel Week 3 Answers 2")

[View Answer](https://my.progiez.com/courses/introduction-to-machine-learning-answers/)

***

**6) Consider a Bayesian classifier for a 5-class classification problem. The following table gives the class-conditioned density fk(x)f\_k(x) for class kk at some point xx in the input space.**

| kk           | 1    | 2    | 3    | 4    | 5    |
| ------------ | ---- | ---- | ---- | ---- | ---- |
| fk(x)f\_k(x) | 0.15 | 0.20 | 0.05 | 0.50 | 0.01 |

Let πk\pi\_k denote the prior probability of class kk. Which of the following statement(s) about the predicted label at xx is/are true?

a) The predicted label at xx will always be class 4.\
b) If 2πi>πi+12\pi\_i > \pi\_{i+1} for all i∈{1,2,3,4}i \in \\{1, 2, 3, 4\\}, the predicted class must be class 4.\
c) If π1>π2>π3>π4\pi\_1 > \pi\_2 > \pi\_3 > \pi\_4, the predicted class must be class 1.\
d) The predicted label at xx can never be class 5.

[View Answer](https://my.progiez.com/courses/introduction-to-machine-learning-answers/)

***

**7) Which of the following statement(s) about a two-class LDA classification model is/are true?**

a) On the decision boundary, the prior probabilities corresponding to both classes must be equal.\
b) On the decision boundary, the posterior probabilities corresponding to both classes must be equal.\
c) On the decision boundary, class-conditioned probability densities corresponding to both classes must be equal.\
d) On the decision boundary, the class-conditioned probability densities corresponding to both classes may or may not be equal.

[View Answer](https://my.progiez.com/courses/introduction-to-machine-learning-answers/)

***

**8) Consider the following two datasets and two LDA classifier models trained respectively on these datasets:**

- **Dataset A:** 200 samples of class 0; 50 samples of class 1.
- **Dataset B:** 200 samples of class 0 (same as Dataset A); 100 samples of class 1 created by repeating twice the class 1 samples from Dataset A.

Let the classifier decision boundary learned be of the form w⋅x+b=0w \cdot x + b = 0, where ww is the slope and bb is the intercept. Which of the given statements is true?

a) The learned decision boundary will be the same for both models.\
b) The two models will have the same slope but different intercepts.\
c) The two models will have different slopes but the same intercept.\
d) The two models may have different slopes and different intercepts.

[View Answer](https://my.progiez.com/courses/introduction-to-machine-learning-answers/)

***

**9) Which of the following statement(s) about LDA is/are true?**

a) It minimizes the inter-class variance relative to the intra-class variance.\
b) It maximizes the inter-class variance relative to the intra-class variance.\
c) Maximizing the Fisher information results in the same direction of the separating hyperplane as the one obtained by equating the posterior probabilities of classes.\
d) Maximizing the Fisher information results in a different direction of the separating hyperplane from the one obtained by equating the posterior probabilities of classes.

[View Answer](https://my.progiez.com/courses/introduction-to-machine-learning-answers/)

***

**10) Which of the following statement(s) regarding logistic regression and LDA is/are true for a binary classification problem?**

a) For any classification dataset, both algorithms learn the same decision boundary.\
b) Adding a few outliers to the dataset is likely to cause a larger change in the decision boundary of LDA compared to that of logistic regression.\
c) Adding a few outliers to the dataset is likely to cause a similar change in the decision boundaries of both classifiers.\
d) If the intra-class distributions deviate significantly from the Gaussian distribution, logistic regression is likely to perform better than LDA.

[View Answer](https://my.progiez.com/courses/introduction-to-machine-learning-answers/)

***


Introduction to Machine Learning Nptel Week 3 Answers (July-Dec 2024)


# Introduction to Machine Learning Nptel Week 3 Answers (July-Dec 2024)<a id="6851"></a>

**Q1**.For a two-class problem using discriminant functions (δ discriminant function for class k\
), where is the separating hyperplane located?

Where δ1>δ2

Where δ1<δ2

Where δ1=δ2

Where δ1+δ2=1

**Answer: True**

**Q2.** Given the following dataset consisting of two classes, A and B ,calculate the prior probability of each class.

What are the prior probabilities of class A and class B ?

P(A)=0.5,P(B)=0.5

P(A)=0.625,P(B)=0.375

P(A)=0.375,P(B)=0.625

P(A)=0.6,P(B)=0.4

**Answer: f(x)=1.5×x+3**

**For answers or latest updates join our telegram channel:** [**Click here to join**](https://telegram.me/nptel_assignments)

**These are Introduction to Machine Learning Nptel Week 2 Answers**

**Q3**.In a 3-class classification problem using linear regression, the output vectors for three data points are \[0.8, 0.3, -0.1], \[0.2, 0.6, 0.2], and \[-0.1, 0.4, 0.7]. To which classes would these points be assigned?\
1, 2, 1\
1, 2, 2\
1, 3, 2\
1, 2, 3

[**Answer: Click here to see answer**](https://progiez.com/introduction-to-machine-learning-nptel-week-3-answers)

**Q4.**If you have a 5-class classification problem and want to avoid masking using polynomial regression, what is the minimum degree of the polynomial you should use?\
3\
4\
5\
6

[**Answer: Click here to see answer**](https://progiez.com/introduction-to-machine-learning-nptel-week-3-answers)

**For answers or latest updates join our telegram channel:** [**Click here to join**](https://telegram.me/nptel_assignments)

**These are Introduction to Machine Learning Nptel Week 2 Answers**

**Q5.** Consider a logistic regression model where the predicted probability for a given data point is 0.4. If the actual label for this data point is 1, what is the contribution of this data point to the log-likelihood?\
-1.3219\
-0.9163\
+1.3219\
+0.9163

[**Answer: Click here to see answer**](https://progiez.com/introduction-to-machine-learning-nptel-week-3-answers)

**Q6.**What additional assumption does LDA make about the covariance matrix in comparison to the basic assumption of Gaussian class conditional density?\
The covariance matrix is diagonal\
The covariance matrix is identity\
The covariance matrix is the same for all classes\
The covariance matrix is different for each class

[**Answer: Click here to see answer**](https://progiez.com/introduction-to-machine-learning-nptel-week-3-answers)

**For answers or latest updates join our telegram channel:** [**Click here to join**](https://telegram.me/nptel_assignments)

**These are Introduction to Machine Learning Nptel Week 2 Answers**

Q7.What is the shape of the decision boundary in LDA?\
Quadratic\
Linear\
Circular\
Can not be determined

[**Answer: Click here to see answer**](https://progiez.com/introduction-to-machine-learning-nptel-week-3-answers)

**Q8.**For two classes C1 and C2 with within-class variances σ2w1=1 and σ2w2=4 respectively, if the projected means are µ′1=1 and µ′2=3 , what is the Fisher criterion J(w) ?\
0.5\
0.8\
1.25\
1.5

[**Answer: Click here to see answer**](https://progiez.com/introduction-to-machine-learning-nptel-week-3-answers)

Q9.Given two classes C1 and C2 with means µ1=\[23] and µ2=\[57] respectively, what is the direction vector w for LDA when the within-class covariance matrix Sw is the identity matrix I ?

\[43]

\[57]

\[0.70.7]

\[0.60.8]

**For answers or latest updates join our telegram channel:** [**Click here to join**](https://telegram.me/nptel_assignments)

**These are Introduction to Machine Learning Nptel Week 2 Answers**

All weeks of Introduction to Machine Learning: [Click Here](https://progiez.com/nptel-assignment-answers/introduction-to-internet-of-things)

For answers to additional Nptel courses, please refer to this link: [NPTEL Assignment Answers](https://progiez.com/nptel-assignment-answers)


# Introduction to Machine Learning Nptel Week 3 Answers (Jan-Apr 2024)<a id="dd53"></a>

**Course name: Introduction to Machine Learning**

**Course Link:** [**Click Here**](https://onlinecourses.nptel.ac.in/noc24_cs51/preview)

**For answers or latest updates join our telegram channel:** [**Click here to join**](https://telegram.me/nptel_assignments)

These are Introduction to Machine Learning Nptel Week 3 Answers

Q1. Which of the following statement(s) about decision boundaries and discriminant functions of classifiers is/are true?\
In a binary classification problem, all points x on the decision boundary satisfy δ1(x)=δ2(x)\
In a three-class classification problem, all points on the decision boundary satisfy δ1(x) = δ2(x) = δ3(x)\
In a three-class classification problem, all points on the decision boundary satisfy at least one of δ1(x) = δ2(x), δ2(x) = δ3(x) or δ3(x) = δ1(x).\
Let the input space be Rn. If x does not lie on the decision boundary, there exists an ϵ>0 such that all inputs y satisfying ||y−x||<ϵ belong to the same class.

**Answer: A, B, D**

**Q2. The following table gives the binary ground truth labels yi for four input points xi (not given). We have a logistic regression model with some parameter values that computes the probability p(xi) that the label is 1. Compute the likelihood of observing the data given these model parameters.**\
0.346\
0.230\
0.058\
0.086

**Answer: 0.230**

**For answers or latest updates join our telegram channel:** [**Click here to join**](https://telegram.me/nptel_assignments)

These are Introduction to Machine Learning Nptel Week 3 Answers

**Q3. Which of the following statement(s) about logistic regression is/are true?**\
It learns a model for the probability distribution of the data points in each class.\
The output of a linear model is transformed to the range (0, 1) by a sigmoid function.\
The parameters are learned by optimizing the mean-squared loss.\
The loss function is optimized by using an iterative numerical algorithm.

**Answer: b, d**

**Q4. Consider a modified form of logistic regression given below where k is a positive constant and β0 and β1 are parameters.\
log(1−p(x)/kp(x))=β0−β1x\
Then find p(x).**\
eβ0/keβ0+eβ1x\
eβ1x/eβ0+keβ1x\
eβ1x/keβ0+eβ1x\
eβ1x/keβ0+e−β1x

**Answer: c. eβ1x/keβ0+eβ1x**

**For answers or latest updates join our telegram channel:** [**Click here to join**](https://telegram.me/nptel_assignments)

These are Introduction to Machine Learning Nptel Week 3 Answers

**Q5. Consider a Bayesian classifier for a 3-class classification problem. The following tables give the class-conditioned density fk(x) for three classes k=1,2,3 at some point x in the input space.\
Note that πk denotes the prior probability of class k. Which of the following statement(s) about the predicted label at x is/are true?**\
If the three classes have equal priors, the prediction must be class 2\
If π3<π2 and π1<π2, the prediction may not necessarily be class 2\
If π1>2π2, the prediction could be class 1 or class 3\
If π1>π2>π3, the prediction must be class 1

**Answer: a, c**

**Q6. The following table gives the binary labels (y(i)) for four points (x(i)1,x(i)2) where i = 1,2,3,4. Among the given options, which set of parameter values β0,β1,β2 of a standard logistic regression model p(xi)=1/1+e−(β0+β1x+β2x) results in the highest likelihood for this data?**\
β0=0.5,β1=1.0,β2=2.0\
β0=−0.5,β1=−1.0,β2=2.0\
β0=0.5,β1=1.0,β2=−2.0\
β0=−0.5,β1=1.0,β2=2.0

**Answer: c. β0=0.5,β1=1.0,β2=−2.0**

**For answers or latest updates join our telegram channel:** [**Click here to join**](https://telegram.me/nptel_assignments)

These are Introduction to Machine Learning Nptel Week 3 Answers
