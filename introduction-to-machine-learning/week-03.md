# NPTEL Introduction To Machine Learning Week 03 Assignment Answers

Are you looking for NPTEL Introduction To Machine Learning Week 03 Assignment Answers? This repository will help you find your answers and solutions for Week 03 of the Introduction To Machine Learning course. We provide detailed solutions to help you complete your assignments efficiently.


![Introduction to Machine Learning Nptel Week 3 Answers (July-Dec 2024)](https://miro.medium.com/v2/resize:fit:875/1*B3u5o95LJrTH3Rv_b6znnA.jpeg)

Introduction to Machine Learning Nptel Week 3 Answers (July-Dec 2024)


# Introduction to Machine Learning Nptel Week 3 Answers (July-Dec 2024)<a id="6851"></a>

**Q1**.For a two-class problem using discriminant functions (δ discriminant function for class k\
), where is the separating hyperplane located?

Where δ1>δ2

Where δ1<δ2

Where δ1=δ2

Where δ1+δ2=1

**Answer: True**

**Q2.** Given the following dataset consisting of two classes, A and B ,calculate the prior probability of each class.

What are the prior probabilities of class A and class B ?

P(A)=0.5,P(B)=0.5

P(A)=0.625,P(B)=0.375

P(A)=0.375,P(B)=0.625

P(A)=0.6,P(B)=0.4

**Answer: f(x)=1.5×x+3**

**For answers or latest updates join our telegram channel:** [**Click here to join**](https://telegram.me/nptel_assignments)

**These are Introduction to Machine Learning Nptel Week 2 Answers**

**Q3**.In a 3-class classification problem using linear regression, the output vectors for three data points are \[0.8, 0.3, -0.1], \[0.2, 0.6, 0.2], and \[-0.1, 0.4, 0.7]. To which classes would these points be assigned?\
1, 2, 1\
1, 2, 2\
1, 3, 2\
1, 2, 3

[**Answer: Click here to see answer**](https://progiez.com/introduction-to-machine-learning-nptel-week-3-answers)

**Q4.**If you have a 5-class classification problem and want to avoid masking using polynomial regression, what is the minimum degree of the polynomial you should use?\
3\
4\
5\
6

[**Answer: Click here to see answer**](https://progiez.com/introduction-to-machine-learning-nptel-week-3-answers)

**For answers or latest updates join our telegram channel:** [**Click here to join**](https://telegram.me/nptel_assignments)

**These are Introduction to Machine Learning Nptel Week 2 Answers**

**Q5.** Consider a logistic regression model where the predicted probability for a given data point is 0.4. If the actual label for this data point is 1, what is the contribution of this data point to the log-likelihood?\
-1.3219\
-0.9163\
+1.3219\
+0.9163

[**Answer: Click here to see answer**](https://progiez.com/introduction-to-machine-learning-nptel-week-3-answers)

**Q6.**What additional assumption does LDA make about the covariance matrix in comparison to the basic assumption of Gaussian class conditional density?\
The covariance matrix is diagonal\
The covariance matrix is identity\
The covariance matrix is the same for all classes\
The covariance matrix is different for each class

[**Answer: Click here to see answer**](https://progiez.com/introduction-to-machine-learning-nptel-week-3-answers)

**For answers or latest updates join our telegram channel:** [**Click here to join**](https://telegram.me/nptel_assignments)

**These are Introduction to Machine Learning Nptel Week 2 Answers**

Q7.What is the shape of the decision boundary in LDA?\
Quadratic\
Linear\
Circular\
Can not be determined

[**Answer: Click here to see answer**](https://progiez.com/introduction-to-machine-learning-nptel-week-3-answers)

**Q8.**For two classes C1 and C2 with within-class variances σ2w1=1 and σ2w2=4 respectively, if the projected means are µ′1=1 and µ′2=3 , what is the Fisher criterion J(w) ?\
0.5\
0.8\
1.25\
1.5

[**Answer: Click here to see answer**](https://progiez.com/introduction-to-machine-learning-nptel-week-3-answers)

Q9.Given two classes C1 and C2 with means µ1=\[23] and µ2=\[57] respectively, what is the direction vector w for LDA when the within-class covariance matrix Sw is the identity matrix I ?

\[43]

\[57]

\[0.70.7]

\[0.60.8]

**For answers or latest updates join our telegram channel:** [**Click here to join**](https://telegram.me/nptel_assignments)

**These are Introduction to Machine Learning Nptel Week 2 Answers**

All weeks of Introduction to Machine Learning: [Click Here](https://progiez.com/nptel-assignment-answers/introduction-to-internet-of-things)

For answers to additional Nptel courses, please refer to this link: [NPTEL Assignment Answers](https://progiez.com/nptel-assignment-answers)


# Introduction to Machine Learning Nptel Week 3 Answers (Jan-Apr 2024)<a id="dd53"></a>

**Course name: Introduction to Machine Learning**

**Course Link:** [**Click Here**](https://onlinecourses.nptel.ac.in/noc24_cs51/preview)

**For answers or latest updates join our telegram channel:** [**Click here to join**](https://telegram.me/nptel_assignments)

These are Introduction to Machine Learning Nptel Week 3 Answers

Q1. Which of the following statement(s) about decision boundaries and discriminant functions of classifiers is/are true?\
In a binary classification problem, all points x on the decision boundary satisfy δ1(x)=δ2(x)\
In a three-class classification problem, all points on the decision boundary satisfy δ1(x) = δ2(x) = δ3(x)\
In a three-class classification problem, all points on the decision boundary satisfy at least one of δ1(x) = δ2(x), δ2(x) = δ3(x) or δ3(x) = δ1(x).\
Let the input space be Rn. If x does not lie on the decision boundary, there exists an ϵ>0 such that all inputs y satisfying ||y−x||<ϵ belong to the same class.

**Answer: A, B, D**

**Q2. The following table gives the binary ground truth labels yi for four input points xi (not given). We have a logistic regression model with some parameter values that computes the probability p(xi) that the label is 1. Compute the likelihood of observing the data given these model parameters.**\
0.346\
0.230\
0.058\
0.086

**Answer: 0.230**

**For answers or latest updates join our telegram channel:** [**Click here to join**](https://telegram.me/nptel_assignments)

These are Introduction to Machine Learning Nptel Week 3 Answers

**Q3. Which of the following statement(s) about logistic regression is/are true?**\
It learns a model for the probability distribution of the data points in each class.\
The output of a linear model is transformed to the range (0, 1) by a sigmoid function.\
The parameters are learned by optimizing the mean-squared loss.\
The loss function is optimized by using an iterative numerical algorithm.

**Answer: b, d**

**Q4. Consider a modified form of logistic regression given below where k is a positive constant and β0 and β1 are parameters.\
log(1−p(x)/kp(x))=β0−β1x\
Then find p(x).**\
eβ0/keβ0+eβ1x\
eβ1x/eβ0+keβ1x\
eβ1x/keβ0+eβ1x\
eβ1x/keβ0+e−β1x

**Answer: c. eβ1x/keβ0+eβ1x**

**For answers or latest updates join our telegram channel:** [**Click here to join**](https://telegram.me/nptel_assignments)

These are Introduction to Machine Learning Nptel Week 3 Answers

**Q5. Consider a Bayesian classifier for a 3-class classification problem. The following tables give the class-conditioned density fk(x) for three classes k=1,2,3 at some point x in the input space.\
Note that πk denotes the prior probability of class k. Which of the following statement(s) about the predicted label at x is/are true?**\
If the three classes have equal priors, the prediction must be class 2\
If π3<π2 and π1<π2, the prediction may not necessarily be class 2\
If π1>2π2, the prediction could be class 1 or class 3\
If π1>π2>π3, the prediction must be class 1

**Answer: a, c**

**Q6. The following table gives the binary labels (y(i)) for four points (x(i)1,x(i)2) where i = 1,2,3,4. Among the given options, which set of parameter values β0,β1,β2 of a standard logistic regression model p(xi)=1/1+e−(β0+β1x+β2x) results in the highest likelihood for this data?**\
β0=0.5,β1=1.0,β2=2.0\
β0=−0.5,β1=−1.0,β2=2.0\
β0=0.5,β1=1.0,β2=−2.0\
β0=−0.5,β1=1.0,β2=2.0

**Answer: c. β0=0.5,β1=1.0,β2=−2.0**

**For answers or latest updates join our telegram channel:** [**Click here to join**](https://telegram.me/nptel_assignments)

These are Introduction to Machine Learning Nptel Week 3 Answers
