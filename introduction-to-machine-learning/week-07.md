# NPTEL Introduction To Machine Learning Week 07 Assignment Answers

Are you looking for NPTEL Introduction To Machine Learning Week 07 Assignment Answers? This repository will help you find your answers and solutions for Week 07 of the Introduction To Machine Learning course. We provide detailed solutions to help you complete your assignments efficiently.

## Introduction to Machine Learning Nptel Week 6 Answers (Jan-Apr 2025)

***

**Que. 1**\
Which of the following statement(s) regarding the evaluation of Machine Learning models is/are true?

a) A model with a lower training loss will perform better on a validation dataset.\
b) A model with a higher training accuracy will perform better on a validation dataset.\
c) The train and validation datasets can be drawn from different distributions.\
d) The train and validation datasets must accurately represent the real distribution of data.

[View Answer](https://my.progiez.com/courses/introduction-to-machine-learning-answers/)

***

**Que. 2**\
Suppose we have a classification dataset comprising of 2 classes A and B with 200 and 40 samples respectively. Suppose we use stratified sampling to split the data into train and test sets. Which of the following train-test splits would be appropriate?

a) Train-{A:50samples, B:10samples}, Test-{A:150samples, B:30samples}\
b) Train-{A:50samples, B:30samples}, Test-{A:150samples, B:10samples}\
c) Train-{A:150samples, B:30samples}, Test-{A:50samples, B:10samples}\
d) Train-{A:150samples, B:10samples}, Test-{A:50samples, B:30samples}

[View Answer](https://my.progiez.com/courses/introduction-to-machine-learning-answers/)

***

**Que. 3**\
Suppose we are performing cross-validation on a multiclass classification dataset with N data points. Which of the following statement(s) is/are correct?

a) In k-fold cross-validation, we train k−1 different models and evaluate them on the same test set.\
b) In k-fold cross-validation, we train k different models and evaluate them on different test sets.\
c) In k-fold cross-validation, each fold should have a class-wise proportion similar to the given dataset.\
d) In LOOCV (Leave-One-Out Cross Validation), we train N different models, using N−1 data points for training each model.

[View Answer](https://my.progiez.com/courses/introduction-to-machine-learning-answers/)

***

**Que. 4**\
For a binary classification problem, we train classifiers and evaluate them to obtain confusion matrices in the following format:\
Which of the following classifiers should be chosen to maximize the recall?

a) \[413677]\
b) \[840260]\
c) \[59581]\
d) \[70390]

[View Answer](https://my.progiez.com/courses/introduction-to-machine-learning-answers/)

***

**Que. 5**\
For the confusion matrices described in Q4, which of the following classifiers should be chosen to minimize the False Positive Rate?

a) \[46684]\
b) \[813277]\
c) \[12988]\
d) \[104086]

[View Answer](https://my.progiez.com/courses/introduction-to-machine-learning-answers/)

***

**Que. 6**\
For the confusion matrices described in Q4, which of the following classifiers should be chosen to maximize the precision?

a) \[46684]\
b) \[813277]\
c) \[12988]\
d) \[104086]

[View Answer](https://my.progiez.com/courses/introduction-to-machine-learning-answers/)

***

**Que. 7**\
For the confusion matrices described in Q4, which of the following classifiers should be chosen to maximize the F1-score?

a) \[46684]\
b) \[83287]\
c) \[12988]\
d) \[104086]

[View Answer](https://my.progiez.com/courses/introduction-to-machine-learning-answers/)

***

**Que. 8**\
Which of the following statement(s) regarding boosting is/are correct?

a) Boosting is an example of an ensemble method.\
b) Boosting assigns equal weights to the predictions of all the weak classifiers.\
c) Boosting may assign unequal weights to the predictions of all the weak classifiers.\
d) The individual classifiers in boosting can be trained parallelly.\
e) The individual classifiers in boosting cannot be trained parallelly.

[View Answer](https://my.progiez.com/courses/introduction-to-machine-learning-answers/)

***

**Que. 9**\
Which of the following statement(s) about bagging is/are correct?

a) Bagging is an example of an ensemble method.\
b) The individual classifiers in bagging can be trained in parallel.\
c) Training sets are constructed from the original dataset by sampling with replacement.\
d) Training sets are constructed from the original dataset by sampling without replacement.\
e) Bagging increases the variance of an unstable classifier.

[View Answer](https://my.progiez.com/courses/introduction-to-machine-learning-answers/)

***

**Que. 10**\
Which of the following statement(s) about ensemble methods is/are correct?

a) Ensemble aggregation methods like bagging aim to reduce overfitting and variance.\
b) Committee machines may consist of different types of classifiers.\
c) Weak learners are models that perform slightly worse than random guessing.\
d) Stacking involves training multiple models and stacking their predictions into new training data.

[View Answer](https://my.progiez.com/courses/introduction-to-machine-learning-answers/)
