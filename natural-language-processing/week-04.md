# NPTEL Natural Language Processing Week 04 Assignment Answers

Are you looking for NPTEL Natural Language Processing Week 04 Assignment Answers? This repository will help you find your answers and solutions for Week 04 of the **Natural Language Processing** course. We provide detailed solutions to help you complete your assignments efficiently.

## Natural Language Processing Nptel Week 4 Quiz Answers (Jan-Apr 2025)

**Course Link: [**Click Here**](https://onlinecourses.nptel.ac.in/noc25_cs51/course)**

***

Q1. Baum-Welch algorithm is an example of –

a) Forward-backward algorithm\
b) Not a case of the Expectation-maximization algorithm\
c) Both A and B\
d) None

[View Answer](https://my.progiez.com/courses/natural-language-processing-nptel-answers/)

***

Q2. Once a day (e.g., at noon), the weather is observed as one of the states:

- State 1: Rainy
- State 2: Cloudy
- State 3: Sunny

The state transition probabilities are given.

Given that the weather on day 1 (t = 1) is sunny (state 3), what is the probability that the weather for the next 7 days will be “sun-sun-rain-rain-sun-cloudy-sun”?

a) 1.54 \* 10-4\
b) 8.9 × 10-2\
c) 7.1 × 10-⁷\
d) 2.5 × 10-10

[View Answer](https://my.progiez.com/courses/natural-language-processing-nptel-answers/)

***

Q3. In question 2, the expected number of consecutive days of sunny weather is:

a) 2\
b) 3\
c) 4\
d) 5

[View Answer](https://my.progiez.com/courses/natural-language-processing-nptel-answers/)

***

Q4. You are building a model distribution for an infinite stream of word tokens. You know that the source of this stream has a vocabulary of size 1200. Out of these 1200 words, you know 200 words to be stop words, each of which has a probability of 0.001.

With only this knowledge, what is the maximum possible entropy of the modeled distribution? (Use log base 10 for entropy calculation.)

a) 2.079\
b) 4.5084\
c) 2.984\
d) 3.0775

[View Answer](https://my.progiez.com/courses/natural-language-processing-nptel-answers/)

***

Q5. Suppose you have the input sentence:\
_“Sachin is a great cricketer.”_

And you know the possible tags each of the words in the sentence can take:

- Sachin: NN, NNS, NNP, NNPS
- is: VB
- a: DT
- great: ADJ
- cricketer: NN, NNS, NNP

[****See also**  **Natural Language Processing Nptel Week 2 Quiz Answers****](https://progiez.com/natural-language-processing-nptel-week-2-quiz-answers)

How many possible hidden state sequences are possible for the above sentence and states?

a) 4 × 3 × 3\
b) 3 × 4\
c) 2¹ × 2³ × 2²\
d) 3 × 4²

[View Answer](https://my.progiez.com/courses/natural-language-processing-nptel-answers/)

***

Q6. What are the time and space complexity order of the Viterbi algorithm? (K is the number of states and N is the number of time steps.)

a) Kⁿ, K2ⁿ\
b) K2ⁿ, Kⁿ\
c) K2N, K2N\
d) Kⁿ, Kⁿ

[View Answer](https://my.progiez.com/courses/natural-language-processing-nptel-answers/)

***

Q7. Mr. X is happy on some days and angry on other days. We can only observe when he smiles, frowns, laughs, or yells but not his actual emotional state. Let us start on day 1 in a happy state. There can be only one state transition per day. It can be either a happy state or an angry state. The HMM is shown below.

Assume that q is the state on day t and oᵗ is the observation on day t.

What is P(oᵗ = frown)?

a) 0.56\
b) 0.18\
c) 0.03\
d) 0.78

[View Answer](https://my.progiez.com/courses/natural-language-processing-nptel-answers/)
