# NPTEL Natural Language Processing Week 02 Assignment Answers

Are you looking for NPTEL Natural Language Processing Week 02 Assignment Answers? This repository will help you find your answers and solutions for Week 02 of the **Natural Language Processing** course. We provide detailed solutions to help you complete your assignments efficiently.

## Natural Language Processing Nptel Week 2 Quiz Answers (Jan-Apr 2026)

**Que.1**\
When computing the Minimum Edit Distance between two strings of length _n_ and _m_ using Dynamic Programming, what are the time and space complexities required for the standard algorithm?

a) Time: O(n + m), Space: O(n)\
b) Time: O(n²), Space: O(1)\
c) Time: O(m), Space: O(n)\
d) Time: O(nm), Space: O(nm)

[View Answer](https://my.progiez.com/courses/natural-language-processing-nptel-answers/)

***

**Que.2**\
Consider a Bigram model with a vocabulary size _V = 10,000_. You encounter a bigram **“smart phone”** which appeared 0 times in the training data. The word **“smart”** appeared 500 times. Using Add-One (Laplace) Smoothing, what is the smoothed probability ( P\_{\text{Add-1}}(\text{phone} \mid \text{smart}) )?

a) 1 / 500\
b) 0 / 500\
c) 1 / 10,500\
d) 1 / 501

[View Answer](https://my.progiez.com/courses/natural-language-processing-nptel-answers/)

***

**Que.3**\
In the context of spelling correction, how are **Non-word errors** distinguished from **Real-word errors**?

a) Non-word errors are grammatical mistakes; Real-word errors are typos\
b) Non-word errors result in a string not found in the dictionary; Real-word errors result in a valid dictionary word\
c) Non-word errors only occur in names; Real-word errors occur in verbs\
d) Non-word errors are handled by N-grams; Real-word errors are handled by Edit Distance only

[View Answer](https://my.progiez.com/courses/natural-language-processing-nptel-answers/)

***

**Que.4**\
For the string **“bread”**, identify which of the following sets of strings has a Levenshtein distance of exactly 1.

a) bead, break, breed, beard, tread\
b) read, broad, dread, bred, spread\
c) read, broad, dread, bred, bead\
d) brand, braid, bead, break, bred

[View Answer](https://my.progiez.com/courses/natural-language-processing-nptel-answers/)

***

**Que.5**\
You are building a Trigram Language Model (_N = 3_). Applying the Markov Assumption, how is the probability of the word ( w\_n ) approximated?

a) ( P(w\_n \mid w\_1, w\_2, \ldots, w\_{n-1}) \approx P(w\_n) )\
b) ( P(w\_n \mid w\_1, w\_2, \ldots, w\_{n-1}) \approx P(w\_n \mid w\_{n-1}) )\
c) ( P(w\_n \mid w\_1, w\_2, \ldots, w\_{n-1}) \approx P(w\_n \mid w\_{n-2}, w\_{n-1}) )\
d) ( P(w\_n \mid w\_1, w\_2, \ldots, w\_{n-1}) \approx P(w\_n \mid w\_{n-3}, w\_{n-2}, w\_{n-1}) )

[View Answer](https://my.progiez.com/courses/natural-language-processing-nptel-answers/)

***

**Que.6**\
Calculate the Levenshtein edit distance between the strings\
S₁ = **“brisk”** and S₂ = **“brick”**, assuming a cost of 1 for insertion, deletion, and substitution. What is the minimum cost and the operation?

a) Cost 1: Substitution\
b) Cost 2: Deletion, Insertion\
c) Cost 1: Transposition\
d) Cost 0: They are identical

[View Answer](https://my.progiez.com/courses/natural-language-processing-nptel-answers/)

***

**Que.7**\
What is the primary motivation for applying Laplace (Add-One) Smoothing to N-gram models?

a) To reduce the perplexity of the training data to zero\
b) To handle the “zero probability” problem\
c) To increase the weight of high-frequency words\
d) To remove stop words from the corpus

[View Answer](https://my.progiez.com/courses/natural-language-processing-nptel-answers/)

***

**Que.8**\
Consider the following training corpus ( C\_1 ) consisting of three sentences (all text lowercased):

S1: “the cat sat on the mat”\
S2: “the dog sat on the log”\
S3: “the cat jumped on the log”

Using Maximum Likelihood Estimation (MLE), calculate the bigram probability ( P(\text{log} \mid \text{the}) ).

a) 2 / 3\
b) 1 / 3\
c) 2 / 5\
d) 1 / 6

[View Answer](https://my.progiez.com/courses/natural-language-processing-nptel-answers/)

***

**Que.9**\
Using the same corpus ( C\_1 ), calculate the **Perplexity** of the bigram model on the test sentence:\
“the cat sat”.

a) √6\
b) 6\
c) √3\
d) 1 / √6

[View Answer](https://my.progiez.com/courses/natural-language-processing-nptel-answers/)

***

**Que.10**\
In the Dynamic Programming algorithm for Minimum Edit Distance, consider a specific cell where _i > 0_ and _j > 0_. The neighboring cells have values:\
D(i−1, j) = 5, D(i, j−1) = 5, and D(i−1, j−1) = 4.\
If the characters X\[i] and Y\[j] are identical (match), and the costs are: insertion = 1, deletion = 1, substitution = 2, match = 0, what is the value of D(i, j)?

a) 4\
b) 5\
c) 6\
d) 3

[View Answer](https://my.progiez.com/courses/natural-language-processing-nptel-answers/)


## Natural Language Processing Nptel Week 2 Quiz Answers (Jan-Apr 2025)

**Course Link: [**Click Here**](https://onlinecourses.nptel.ac.in/noc25_cs51/course)**

***

1\. According to Zipf’s law which statements) is/are correct?\
(i) A small number of words occur with high frequency.\
(ii) A large number of words occur with low frequency.

a. Both (i) and (ii) are correct\
b. Only (ii) is correct\
c. Only (i) is correct\
d. Neither (i) nor (ii) is correct

******[**View Answer**](https://my.progiez.com/courses/natural-language-processing-nptel-answers/)******

***

2. **Consider the following corpus Ci of 4 sentences. What is the total count of unique bi-grams for which the likelihood will be estimated? Assume we do not perform any pre-processing.**

tomorrow is Sachin’s birthday

He loves cream chocolates

he is also fond of sweet cake

we will celebrate his birthday with sweet chocolate cake

today is Sneha’s birthday

she likes ice cream

she is also fond of cream cake

we will celebrate her birthday with ice cream cake

a. 24

b. 28

c. 27

d. 23

******[**View Answer**](https://my.progiez.com/courses/natural-language-processing-nptel-answers/)******

***

3\. A 4-gram model is a\_\_\_\_\_\_\_\_\_\_\_\_\_\_order Markov Model.

a. Two\
b. Five\
c. Four\
d. Three

******[**View Answer**](https://my.progiez.com/courses/natural-language-processing-nptel-answers/)******

***

4\. Which of these is/are – valid Markov assumptions?

a. The probability of a word depends only on the current word.\
b. The probability of a word depends only on the previous word.\
c. The probability of a word depends only on the next word.\
d. The probability of a word depends only on the current and the previous word.

[****See also**  **Natural Language Processing Nptel Week 1 Quiz Answers****](https://progiez.com/natural-language-processing-nptel-week-1-quiz-answers)

******[**View Answer**](https://my.progiez.com/courses/natural-language-processing-nptel-answers/)******

***

5\. For the string ‘mash’, identify which of the following set of strings has a Levenshtein distance of\
1\.

a. smash, mas, lash, mushy, hash\
b. bash, stash, lush, flash, dash\
c. smash, mas, lash, mush, ash\
d. None of the above

******[**View Answer**](https://my.progiez.com/courses/natural-language-processing-nptel-answers/)******

***

6\. Assume that we modify the costs incurred for operations in calculating Levenshtein distance,\
such that both the insertion and deletion operations incur a cost of 1 each, while substitution\
incurs a cost of 2. Now, for the string ‘clash’ which of the following set of strings will have an\
edit distance of 1?

a. ash, slash, clash, flush\
b. flash, stash, lush, blush,\
c. slash, last, bash, ash\
d. None of the above

******[**View Answer**](https://my.progiez.com/courses/natural-language-processing-nptel-answers/)******

***

7\. Given a corpus C2, the Maximum Likelihood Estimation (MLE) for the bigram “dried berries” is\
0.45 and the count of occurrence of the word “dried” is 720. For the same corpus C the likelihood\
of “dried berries” after applying add-one smoothing is 0.05. What is the vocabulary size of C2?

a. 4780\
b. 3795\
c. 4955\
d. 5780

******[**View Answer**](https://my.progiez.com/courses/natural-language-processing-nptel-answers/)******

***

8\. Calculate P(they play in a big garden) assuming a bi-gram language model.

a. 1/8\
b. 1/12\
c. 1/24\
d. None of the above

******[**View Answer**](https://my.progiez.com/courses/natural-language-processing-nptel-answers/)******

***

9\. Considering the same model as in Question 7, calculate the perplexity of they play in a big\
garden < |s>.

a. 2.289\
b. 1.426\
C. 1.574\
d. 2.178

******[**View Answer**](https://my.progiez.com/courses/natural-language-processing-nptel-answers/)******

***

10\. Assume that you are using a bi-gram language model with add one smoothing. Calculate P(they play in a beautiful garden).

[****See also**  **Natural Language Processing Nptel Week 1 Quiz Answers****](https://progiez.com/natural-language-processing-nptel-week-1-quiz-answers)

a. 4.472 × 101-6\
b. 2.236 × 10^-6\
c. 3.135 × 10^-6\
d. None of the above

****[**View Answer**](https://my.progiez.com/courses/natural-language-processing-nptel-answers/)****

***

_Natural Language Processing Nptel Week 2 Quiz Answers_

For answers to others Nptel courses, please refer to this link: [NPTEL Assignment](https://progiez.com/nptel-assignment-answers)
