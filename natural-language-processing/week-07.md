# NPTEL Natural Language Processing Week 07 Assignment Answers

Are you looking for NPTEL Natural Language Processing Week 07 Assignment Answers? This repository will help you find your answers and solutions for Week 07 of the **Natural Language Processing** course. We provide detailed solutions to help you complete your assignments efficiently.

## Natural Language Processing Nptel Week 7 Quiz Answers (Jan-Apr 2025)

**Course Link: [**Click Here**](https://onlinecourses.nptel.ac.in/noc25_cs51/course)**

***

**Que. 1**\
Suppose you have a raw text corpus and you compute a word co-occurrence matrix from there. Which of the following algorithm(s) can you utilize to learn word representations? (Choose all that apply)

a) CBOW\
b) SVD\
c) PCA\
d) GloVe

[View Answer](https://my.progiez.com/courses/natural-language-processing-nptel-answers/)

***

**Que. 2**\
What is the method for solving word analogy questions like, given A, B, and D, find C such that A:B::C:D, using word vectors?

a) Vc=Va+(Vb−Va)V\_c = V\_a + (V\_b – V\_a), then use cosine similarity to find the closest word of VcV\_c.\
b) Vc=Va+(Vd−Vb)V\_c = V\_a + (V\_d – V\_b), then do dictionary lookup for VcV\_c.\
c) Vc=Va+(Va−Vp)V\_c = V\_a + (V\_a – V\_p), then use cosine similarity to find the closest word of VcV\_c.\
d) Vc=Va+(Va−Vb)V\_c = V\_a + (V\_a – V\_b), then do dictionary lookup for VcV\_c.\
e) None of the above

[View Answer](https://my.progiez.com/courses/natural-language-processing-nptel-answers/)

***

**Que. 3**\
What is the value of PMI(w1,w2)PMI(w\_1, w\_2) for C(w1)=100C(w\_1) = 100, C(w2)=2500C(w\_2) = 2500, C(w1,w2)=320C(w\_1, w\_2) = 320, N=50000N = 50000?

NN: Total number of documents.\
C(w1)C(w\_1): Number of documents where w1w\_1 has appeared.\
C(w1,w2)C(w\_1, w\_2): Number of documents where both words have appeared.

**Note:** Use base 2 in logarithm.

a) 4\
b) 5\
c) 6\
d) 5.64

[View Answer](https://my.progiez.com/courses/natural-language-processing-nptel-answers/)

***

**Que. 4**\
Given two binary word vectors w1w\_1 and w2w\_2 as follows:

w1=\[1010011010]w\_1 = \[1010011010]\
w2=\[0011111100]w\_2 = \[0011111100]

Compute the Dice and Jaccard similarity between them.

[****See also**  **Natural Language Processing Nptel Week 2 Quiz Answers****](https://progiez.com/natural-language-processing-nptel-week-2-quiz-answers)

a) 6/11, 3/8\
b) 10/11, 5/6\
c) 4/9, 2/7\
d) 5/9, 5/8

[View Answer](https://my.progiez.com/courses/natural-language-processing-nptel-answers/)

***

**Que. 5**\
Consider two probability distributions for two words pp and qq. Compute their similarity scores with KL-divergence.

p=\[0.20,0.75,0.50]p = \[0.20, 0.75, 0.50]\
q=\[0.90,0.10,0.25]q = \[0.90, 0.10, 0.25]

**Note:** Use base 2 in logarithm.

a) 4.704, 1.720\
b) 1.692, 0.553\
c) 2.246, 1.412\
d) 3.213, 2.426

[View Answer](https://my.progiez.com/courses/natural-language-processing-nptel-answers/)

***

**Que. 6**\
Consider the following word co-occurrence matrix given below. Compute the cosine similarity between:

(i) w1w\_1 and w2w\_2, and\
(ii) w1w\_1 and w3w\_3.

|        | w4w\_4 | w5w\_5 | w6w\_6 |
| ------ | ------ | ------ | ------ |
| w1w\_1 | 2      | 8      | 5      |
| w2w\_2 | 4      | 9      | 7      |
| w3w\_3 | 1      | 2      | 3      |

a) 0.773, 0.412\
b) 0.881, 0.764\
c) 0.987, 0.914\
d) 0.897, 0.315

[View Answer](https://my.progiez.com/courses/natural-language-processing-nptel-answers/)

***

**Que. 7**

‘Which ofthe folowing types of relations can be captured by word2vec (CBOW or\
Skipgram)?

1. Analogy (A:B::C:?)
2. Antonymy
3. Polysemy
4. All of the above

[View Answer](https://my.progiez.com/courses/natural-language-processing-nptel-answers/)
