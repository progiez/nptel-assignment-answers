# NPTEL Introduction To Machine Learning IIT KGP Week 02 Assignment Answers

Are you looking for NPTEL Introduction To Machine Learning IIT KGP Week 02 Assignment Answers? This repository will help you find your answers and solutions for Week 02 of the Introduction To Machine Learning IIT KGP course. We provide detailed solutions to help you complete your assignments efficiently.

![](https://miro.medium.com/v2/resize:fit:875/1*nh2U8GRaYnyFlaeRQVDh6g.jpeg)

## Introduction To Machine Learning IIT-KGP Week 1 Answers (July-Dec 2025)

***

**Que.1 Which of the following are classification tasks?**\
A) Find the gender of a person by analyzing his writing style\
B) Predict the price of a house based on floor area, number of rooms etc.\
C) Predict the temperature for the next day\
D) Predict the number of copies of a book that will be sold this month

[View Answer](https://my.progiez.com/courses/introduction-to-machine-learning-iitkgp-nptel-answers/)

***

**Que.2 Which of the following is not a categorical feature?**\
A) Gender of a person\
B) Height of a person\
C) Types of Mountains\
D) Nationality of a person

[View Answer](https://my.progiez.com/courses/introduction-to-machine-learning-iitkgp-nptel-answers/)

***

**Que.3 Which of the following tasks is NOT a suitable machine learning task?**\
A) Finding the shortest path between a pair of nodes in a graph\
B) Predicting if a stock price will rise or fall\
C) Predicting the price of petroleum\
D) Grouping mails as spams or non-spams

[View Answer](https://my.progiez.com/courses/introduction-to-machine-learning-iitkgp-nptel-answers/)

***

**Que.4 Suppose I have 10,000 emails in my mailbox out of which 200 are spams. The spam detection system detects 150 mails as spams, out of which 50 are actually spams. What is the precision and recall of my spam detection system?**\
A) Precision = 33.333%, Recall = 25%\
B) Precision = 25%, Recall = 33.33%\
C) Precision = 33.33%, Recall = 75%\
D) Precision = 75%, Recall = 33.33%

[View Answer](https://my.progiez.com/courses/introduction-to-machine-learning-iitkgp-nptel-answers/)

***

**Que.5 A feature F1 can take certain values: A, B, C, D, E, F and represents the grade of students from a college. Which of the following statements is true?**\
A) Feature F1 is an example of a nominal variable.\
B) Feature F1 is an example of ordinal variables.\
C) It doesn’t belong to any of the above categories.\
D) Both of these

[****See also**  **Entrepreneurship Week 1 Nptel Assignment Answers****](https://progiez.com/entrepreneurship-week-1-nptel-assignment-answers)

[View Answer](https://my.progiez.com/courses/introduction-to-machine-learning-iitkgp-nptel-answers/)

***

**Que.6 Which learning paradigm is most suitable for training a robotic painting arm that must paint every corner while minimizing paint waste?**\
A) Supervised learning\
B) Unsupervised learning\
C) Combination of supervised and unsupervised learning\
D) Reinforcement learning

[View Answer](https://my.progiez.com/courses/introduction-to-machine-learning-iitkgp-nptel-answers/)

***

**Que.7 How many Boolean functions are possible with n features?**\
A) 2^{2^N}\
B) 2^N\
C) N^2\
D) 4^N

[View Answer](https://my.progiez.com/courses/introduction-to-machine-learning-iitkgp-nptel-answers/)

***

**Que.8 What is the use of Validation dataset in Machine Learning?**\
A) To train the machine learning model\
B) To evaluate the performance of the machine learning model\
C) To tune the hyperparameters of the machine learning model\
D) None of the above

[View Answer](https://my.progiez.com/courses/introduction-to-machine-learning-iitkgp-nptel-answers/)

***

**Que.9 Regarding bias and variance, which of the following statements are true?**\
A) Models which overfit have a high bias.\
B) Models which overfit have a low bias.\
C) Models which underfit have a high variance.\
D) Models which underfit have a low variance.

[View Answer](https://my.progiez.com/courses/introduction-to-machine-learning-iitkgp-nptel-answers/)

***

**Que.10 Identify whether the following statement is true or false? “Occam’s Razor is an example of Inductive Bias”**\
A) True\
B) False

[View Answer](https://my.progiez.com/courses/introduction-to-machine-learning-iitkgp-nptel-answers/)



Introduction To Machine Learning IIT-KGP Week 2 Answers (July-Dec 2024)


# Introduction To Machine Learning IIT-KGP Week 2 Answers (July-Dec 2024)<a id="bc5a"></a>

**Q1.**In a binary classification problem, out of 30 data points 10 belong to class I and 20 belong to class II. What is the entropy of the data set?\
Α. 0.97\
B. 0\
C. 0.91\
D. 0.67

**Answer: C. 0.91**

**Q2.**Which of the following is false?\
A. Bias is the true error of the best classifier in the concept class\
B. Bias is high if the concept class cannot model the true data distribution well\
C. High bias leads to overfitting

**Answer: C. High bias leads to overfitting**

**For answers or latest updates join our telegram channel:** [**Click here to join**](https://telegram.me/nptel_assignments)

**These are Introduction To Machine Learning IIT-KGP Week 2 Answers**

**Q3**.Decision trees can be used for the problems where

1. the attributes are categorical.
2. the attributes are numeric valued.
3. the attributes are discrete valued.\
   A. 1 only\
   B. 1 and 2 only\
   C. 1, 2 and 3

[**_**Answer: Click here to See answer**_**](https://progiez.com/introduction-to-machine-learning-iit-kgp-week-2-answers)

**Q4.**In linear regression, our hypothesis is is h(x) = 0 + 01 0₁x, the training data is given in the 0 1 table.\
m Σ((x)- i=1 θ If the cost function is J(0) = 1 2m What is the value of J(0) when 8 = (1,1) ? 2 y), where m is no. of training data points.\
A. 0\
B. 2\
C. 1\
D. 0.25

[**_**Answer: Click here to See answer**_**](https://progiez.com/introduction-to-machine-learning-iit-kgp-week-2-answers)

**For answers or latest updates join our telegram channel:** [**Click here to join**](https://telegram.me/nptel_assignments)

**These are Introduction To Machine Learning IIT-KGP Week 2 Nptel Assignment Answers**

**Q5.**The value of information gain in the following decision tree is:\
Entropy 0.996) Examples=30\
Entropy 0.787 Examples=17\
Entropy=0.391 Examples 13\
Α. 0.380\
Β. 0.620\
C. 0.190\
D. 0.477

[**_**Answer: Click here to See answer**_**](https://progiez.com/introduction-to-machine-learning-iit-kgp-week-2-answers)

**Q6** .What is true for Stochastic Gradient Descent?\
A. In every iteration, model parameters are updated based on multiple training samples.\
B. In every iteration, model parameters are updated based on one training sample\
C. In every iteration, model parameters are updated based on all training samples\
D. None of the above

[**_**Answer: Click here to See answer**_**](https://progiez.com/introduction-to-machine-learning-iit-kgp-week-2-answers)

**For answers or latest updates join our telegram channel:** [**Click here to join**](https://telegram.me/nptel_assignments)

**These are Introduction To Machine Learning IIT-KGP Week 2 Nptel Assignment Answers**

Answer Questions 7–8 with the data given below:\
ISRO wants to discriminate between Martians (M) and Humans (H) based on the following features: Green ∈ {N,Y), Legs ∈ {2,3}, Height ∈ {S,T), Smelly ∈ {N,Y). The decision variable is Species. The training data is as follows:

Q7.The entropy of the entire dataset is\
A. 0.5\
B. 1\
C. 0\
D. 0.1

[**_**Answer: Click here to See answer**_**](https://progiez.com/introduction-to-machine-learning-iit-kgp-week-2-answers)

**Q8.**Which attribute will be the root of the decision tree (if information gain is used to create the decision tree) and what is the information gain due to that attribute?\
A. Green, 0.45\
B. Legs, 0.4\
C. Height, 0.8\
D. Smelly,0.7

[**_**Answer: Click here to See answer**_**](https://progiez.com/introduction-to-machine-learning-iit-kgp-week-2-answers)

**For answers or latest updates join our telegram channel:** [**Click here to join**](https://telegram.me/nptel_assignments)

**These are Introduction To Machine Learning IIT-KGP Week 2 Nptel Assignment Answers**

**Q9.**In Linear Regression the output is:\
A. Discrete\
B. Continuous and always lies in a finite range\
C. Continuous\
D. May be discrete or continuous

[**_**Answer: Click here to See answer**_**](https://progiez.com/introduction-to-machine-learning-iit-kgp-week-2-answers)

**For answers or latest updates join our telegram channel:** [**Click here to join**](https://telegram.me/nptel_assignments)

**These are Introduction To Machine Learning IIT-KGP Week 2 Nptel Assignment Answers**

**Q10.**Identify whether the following statement is true or false?\
“Overfitting is more likely when the set of training data is small”\
A. True\
B. False

[**_**Answer: Click here to See answer**_**](https://progiez.com/introduction-to-machine-learning-iit-kgp-week-2-answers)

**For answers or latest updates join our telegram channel:** [**Click here to join**](https://telegram.me/nptel_assignments)

**These are Introduction To Machine Learning IIT-KGP Week 2 Nptel Assignment Answers**

All weeks of Introduction to Machine Learning: [Click Here](https://progiez.com/nptel-assignment-answers/introduction-to-machine-learning)

More Nptel Courses: <https://progiez.com/nptel-assignment-answers>


# Introduction To Machine Learning IIT-KGP Week 2 Answers (Jan-Apr 2024)<a id="c4b5"></a>

Link to Enroll: [Click Here](https://onlinecourses.nptel.ac.in/noc22_cs29/preview)

**1. In a binary classification problem, out of 30 data points 12 belong to class I and 18 belong to class II. What is the entropy of the data set?**

A. 0.97\
B 0\
**C. 1**D. 0.67

**Answer:- c**

**2. Decision trees can be used for the problems where**

A. the attributes are categorical.\
B. the attributes are numeric valued.\
C. the attributes are discrete valued.\
**D. In all the above cases.**

**Answer:- d**

**3. Which of the following is false?**

A. Variance is the error of the trained classifier with respect to the best classifier in the concept class.\
B. Variance depends on the training set size.\
**C. Variance increases with more training data.**D. Variance increases with more complicated classifiers.

**Answer:- c**

**4. In linear regression, our hypothesis is h (x) = 6+ 0x, the training data is given in the table. A**What is the value of J(0) when 6 = (1,1).

A. 0\
B. 1\
C. 2\
D. 0.5

**Answer:- b**
