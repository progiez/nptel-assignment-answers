# NPTEL Introduction To Machine Learning IIT KGP Week 03 Assignment Answers

Are you looking for NPTEL Introduction To Machine Learning IIT KGP Week 03 Assignment Answers? This repository will help you find your answers and solutions for Week 03 of the Introduction To Machine Learning IIT KGP course. We provide detailed solutions to help you complete your assignments efficiently.


## _Introduction to Machine Learning Nptel Week 3 Answers (Jan-Apr 2025)_

***

**1) Which of the following statement(s) about decision boundaries and discriminant functions of classifiers is/are true?**

a) In a binary classification problem, all points xx on the decision boundary satisfy δ1(x)=δ2(x)\delta\_1(x) = \delta\_2(x).\
b) In a three-class classification problem, all points on the decision boundary satisfy δ1(x)=δ2(x)=δ3(x)\delta\_1(x) = \delta\_2(x) = \delta\_3(x).\
c) In a three-class classification problem, all points on the decision boundary satisfy at least one of δ1(x)=δ2(x)\delta\_1(x) = \delta\_2(x), δ2(x)=δ3(x)\delta\_2(x) = \delta\_3(x), δ3(x)=δ1(x)\delta\_3(x) = \delta\_1(x).\
d) If xx does not lie on the decision boundary, then all points lying in a sufficiently small neighborhood around xx belong to the same class.

[View Answer](https://my.progiez.com/courses/introduction-to-machine-learning-answers/)

***

**2) You train an LDA classifier on a dataset with 2 classes. The decision boundary is significantly different from the one obtained by logistic regression. What could be the reason?**

a) The underlying data distribution is Gaussian.\
b) The two classes have equal covariance matrices.\
c) The underlying data distribution is not Gaussian.\
d) The two classes have unequal covariance matrices.

[View Answer](https://my.progiez.com/courses/introduction-to-machine-learning-answers/)

***

**3) The following table gives the binary ground truth labels YiY\_i for four input points (not given). We have a logistic regression model with some parameter values that computes the probability P1(Xi)P\_1(X\_i) that the label is 1. Compute the likelihood of observing the data given these model parameters.**

| YiY\_i           | 1   | 0   | 1   | 0   |
| ---------------- | --- | --- | --- | --- |
| P1(Xi)P\_1(X\_i) | 0.8 | 0.5 | 0.2 | 0.9 |

a) 0.072\
b) 0.144\
c) 0.288\
d) 0.002

[View Answer](https://my.progiez.com/courses/introduction-to-machine-learning-answers/)

***

**4) Which of the following statement(s) about logistic regression is/are true?**

a) It learns a model for the probability distribution of the data points in each class.\
b) The output of a linear model is transformed to the range (0,1) by a sigmoid function.\
c) The parameters are learned by minimizing the mean-squared loss.\
d) The parameters are learned by maximizing the log-likelihood.

[View Answer](https://my.progiez.com/courses/introduction-to-machine-learning-answers/)

***

**5) Consider a modified form of logistic regression given below where kk is a positive constant and β0\beta\_0, β1\beta\_1 are parameters:**

![image 26](https://progiez.com/wp-content/uploads/2024/08/image-26.png "Introduction to Machine Learning Nptel Week 3 Answers 2")

[View Answer](https://my.progiez.com/courses/introduction-to-machine-learning-answers/)

***

**6) Consider a Bayesian classifier for a 5-class classification problem. The following table gives the class-conditioned density fk(x)f\_k(x) for class kk at some point xx in the input space.**

| kk           | 1    | 2    | 3    | 4    | 5    |
| ------------ | ---- | ---- | ---- | ---- | ---- |
| fk(x)f\_k(x) | 0.15 | 0.20 | 0.05 | 0.50 | 0.01 |

Let πk\pi\_k denote the prior probability of class kk. Which of the following statement(s) about the predicted label at xx is/are true?

a) The predicted label at xx will always be class 4.\
b) If 2πi>πi+12\pi\_i > \pi\_{i+1} for all i∈{1,2,3,4}i \in \\{1, 2, 3, 4\\}, the predicted class must be class 4.\
c) If π1>π2>π3>π4\pi\_1 > \pi\_2 > \pi\_3 > \pi\_4, the predicted class must be class 1.\
d) The predicted label at xx can never be class 5.

[View Answer](https://my.progiez.com/courses/introduction-to-machine-learning-answers/)

***

**7) Which of the following statement(s) about a two-class LDA classification model is/are true?**

a) On the decision boundary, the prior probabilities corresponding to both classes must be equal.\
b) On the decision boundary, the posterior probabilities corresponding to both classes must be equal.\
c) On the decision boundary, class-conditioned probability densities corresponding to both classes must be equal.\
d) On the decision boundary, the class-conditioned probability densities corresponding to both classes may or may not be equal.

[View Answer](https://my.progiez.com/courses/introduction-to-machine-learning-answers/)

***

**8) Consider the following two datasets and two LDA classifier models trained respectively on these datasets:**

- **Dataset A:** 200 samples of class 0; 50 samples of class 1.
- **Dataset B:** 200 samples of class 0 (same as Dataset A); 100 samples of class 1 created by repeating twice the class 1 samples from Dataset A.

Let the classifier decision boundary learned be of the form w⋅x+b=0w \cdot x + b = 0, where ww is the slope and bb is the intercept. Which of the given statements is true?

a) The learned decision boundary will be the same for both models.\
b) The two models will have the same slope but different intercepts.\
c) The two models will have different slopes but the same intercept.\
d) The two models may have different slopes and different intercepts.

[View Answer](https://my.progiez.com/courses/introduction-to-machine-learning-answers/)

***

**9) Which of the following statement(s) about LDA is/are true?**

a) It minimizes the inter-class variance relative to the intra-class variance.\
b) It maximizes the inter-class variance relative to the intra-class variance.\
c) Maximizing the Fisher information results in the same direction of the separating hyperplane as the one obtained by equating the posterior probabilities of classes.\
d) Maximizing the Fisher information results in a different direction of the separating hyperplane from the one obtained by equating the posterior probabilities of classes.

[View Answer](https://my.progiez.com/courses/introduction-to-machine-learning-answers/)

***

**10) Which of the following statement(s) regarding logistic regression and LDA is/are true for a binary classification problem?**

a) For any classification dataset, both algorithms learn the same decision boundary.\
b) Adding a few outliers to the dataset is likely to cause a larger change in the decision boundary of LDA compared to that of logistic regression.\
c) Adding a few outliers to the dataset is likely to cause a similar change in the decision boundaries of both classifiers.\
d) If the intra-class distributions deviate significantly from the Gaussian distribution, logistic regression is likely to perform better than LDA.

[View Answer](https://my.progiez.com/courses/introduction-to-machine-learning-answers/)

***


Introduction To Machine Learning IIT-KGP Week 3 Answers (July-Dec 2024)


# Introduction To Machine Learning IIT-KGP Week 3 Answers (July-Dec 2024)<a id="d4e5"></a>

**Q1.**What will be the class of a new data point x1=1 and x2=1 in 5-NN (k nearest neighbour with k=5) using euclidean distance measure?\
A. + Class\
B. — Class\
C. Cannot be determined

**Answer: C. 0.91**

**Q2.**Imagine you are dealing with a 10 class classification problem. What is the maximum number of discriminant vectors that can be produced by LDA?\
A. 20\
B. 14\
C. 9\
D. 10

C. High bias leads to overfitting

**Answer: C. High bias leads to overfitting**

**For answers or latest updates join our telegram channel:** [**Click here to join**](https://telegram.me/nptel_assignments)

**These are Introduction To Machine Learning IIT-KGP Week 3 Answers**

**Q3**.Fill in the blanks:\
K-Nearest Neighbor is a\
algorithm\
A. Non-parametric, eager\
B. Parametric, eager\
C. Non-parametric, lazy\
D. Parametric, lazy

[**Answer: CLick here to see answer**](https://progiez.com/introduction-to-machine-learning-iit-kgp-week-3-answers)

**Q4**.Which of the following statements is True about the KNN algorithm?\
A. KNN algorithm does more computation on test time rather than train time.\
B. KNN algorithm does lesser computation on test time rather than train time.\
C. KNN algorithm does an equal amount of computation on test time and train time.\
D. None of these.

[**Answer: CLick here to see answer**](https://progiez.com/introduction-to-machine-learning-iit-kgp-week-3-answers)

**For answers or latest updates join our telegram channel:** [**Click here to join**](https://telegram.me/nptel_assignments)

**These are Introduction To Machine Learning IIT-KGP Week 3 Answers**

**Q5.**Which of the following necessitates feature reduction in machine learning?

Limited computational resources.\
A. 1 only\
B. 2 only\
C. 1 and 2 only\
D. 1, 2 and 3

[**Answer: CLick here to see answer**](https://progiez.com/introduction-to-machine-learning-iit-kgp-week-3-answers)

**Q6** .When there is noise in data, which of the following options would improve the performance of the k-NN algorithm?\
A. Increase the value of k\
B. Decrease the value of k\
C. Changing value of k will not change the effect of the noise\
D. None of these

[**Answer: CLick here to see answer**](https://progiez.com/introduction-to-machine-learning-iit-kgp-week-3-answers)

**For answers or latest updates join our telegram channel:** [**Click here to join**](https://telegram.me/nptel_assignments)

**These are Introduction To Machine Learning IIT-KGP Week 3 Answers**

Q7.Find the value of the Pearson’s correlation coefficient of X and Y from the data in the following table.

Α. 0.47\
B. 0.68\
C. 1\
D. 0.33

[**Answer: CLick here to see answer**](https://progiez.com/introduction-to-machine-learning-iit-kgp-week-3-answers)

**Q8.**Which of the following statements is/are true about PCA?

1. PCA is a supervised method
2. It identifies the directions that data have the largest variance
3. Maximum number of principal components <= number of features
4. All principal components are orthogonal to each other

A. Only 2\
B. 1, 3 and 4\
C. 1, 2 and 3\
D. 2, 3 and 4

[**Answer: CLick here to see answer**](https://progiez.com/introduction-to-machine-learning-iit-kgp-week-3-answers)

**For answers or latest updates join our telegram channel:** [**Click here to join**](https://telegram.me/nptel_assignments)

**These are Introduction To Machine Learning IIT-KGP Week 3 Answers**

**Q9.**In user-based collaborative filtering based recommendation, the items are recommended based on:\
A. Similar users\
B. Similar items\
C. Both of the above\
D. None of the above

[**Answer: CLick here to see answer**](https://progiez.com/introduction-to-machine-learning-iit-kgp-week-3-answers)

**For answers or latest updates join our telegram channel:** [**Click here to join**](https://telegram.me/nptel_assignments)

**These are Introduction To Machine Learning IIT-KGP Week 3 Answers**

**Q10**.Identify whether the following statement is true or false? “Linear Discriminant Analysis (LDA) is a supervised method”\
A. TRUE\
B. FALSE

[**Answer: CLick here to see answer**](https://progiez.com/introduction-to-machine-learning-iit-kgp-week-3-answers)

**For answers or latest updates join our telegram channel:** [**Click here to join**](https://telegram.me/nptel_assignments)

**These are Introduction To Machine Learning IIT-KGP Week 3 Answers**

All weeks of Introduction to Machine Learning: [Click Here](https://progiez.com/nptel-assignment-answers/introduction-to-machine-learning)

More Nptel Courses: <https://progiez.com/nptel-assignment-answers>


# Introduction To Machine Learning IIT-KGP Week 3 Answers (July-Dec 2022)<a id="7933"></a>

Course Name: Introduction To Machine Learning IITKGP

Link to Enroll: [Click Here](https://onlinecourses.nptel.ac.in/noc22_cs29/preview)

**Q1. Suppose, you have given the following data where x and y are the 2 input variables and Class is the dependent variable.**\
Suppose, you want to predict the class of new data point x=1 and y=1 using euclidean distance in 3-NN. To which class the new data point belongs to?

a. +Class\
b. -Class\
c. Can’t say\
d. None of these

**Answer: b. — Class**

**Q2. Imagine you are dealing with a 10 class classification problem. What is the maximum number of discriminant vectors that can be produced by LDA?**

a. 20\
b. 14\
c. 9\
d. 10

**Answer: c. 9**

**These are Introduction To Machine Learning IIT-KGP Week 3 Answers**

**Q3. Fill in the blanks: K-Nearest Neighbor is a\_ algorithm**\
a. Non-parametric, eager\
b. Parametric, eager\
c. Non-parametric, lazy\
d. Parametric, lazy

**Answer: c. Non-parametric, lazy**

**Q4. Which of the following statements is True about the KNN algorithm?**\
a. KNN algorithm does more computation on test time rather than train time.\
b. KNN algorithm does lesser computation on test time rather than train time.\
c. KNN algorithm does an equal amount of computation on test time and train time.\
d. None of these.

**Answer: a. KNN algorithm does more computation on test time rather than train time.**

**These are Introduction To Machine Learning IIT-KGP Week 3 Answers**

**Q5. Which of the following necessitates feature reduction in machine learning?**\
A. Irrelevant and redundant features\
B. Curse of dimensionality\
C. Limited computational resources.\
D. All of the above

**Answer: d. All of the above**

**Q6. When there is noise in data, which of the following options would improve the perfomance of the KNN algorithm?**\
a. Increase the value of k\
b. Decrease the value of k\
c. Changing value of k will not change the effect of the noise\
d. None of these

**Answer: a. Increase the value of k**

**These are Introduction To Machine Learning IIT-KGP Week 3 Answers**

**Q7. Find the value of the Pearson’s correlation coefficient of X and Y from the data in the following table.**\
a. 0.47\
b. 0.68\
c. 1\
d. 0.33

**Answer: b. 0.68**

**Q8. Which of the following is false about PCA?**\
a. PCA is a supervised method\
b. It identifies the directions that data have the largest variance\
c. Maximum number of principal components = number of features\
d. All principal components are othogonal to each other

**Answer: a. PCA is a supervised method**

**Q9. In user-based collaborative filtering based recommendation, the items are recommended based on :**\
a. Similar users\
b. Similar items\
c. Both of the above\
d. None of the above

**Answer: a. Similar users**

**These are Introduction To Machine Learning IIT-KGP Week 3 Answers**

**Q10. Identify whether the following statement is true or false? “PCA can be used for projecting and visualizing data in lower dimensions.”**\
a. True\
b. False

**Answer: a. True**
