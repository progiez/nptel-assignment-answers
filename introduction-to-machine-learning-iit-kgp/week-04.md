# NPTEL Introduction To Machine Learning IIT KGP Week 04 Assignment Answers

Are you looking for NPTEL Introduction To Machine Learning IIT KGP Week 04 Assignment Answers? This repository will help you find your answers and solutions for Week 04 of the Introduction To Machine Learning IIT KGP course. We provide detailed solutions to help you complete your assignments efficiently.


![Introduction To Machine Learning IIT-KGP Week 4 Answers (July-Dec 2024)](https://miro.medium.com/v2/resize:fit:875/1*wnVpJMHAyOdLYrwJUQ_4bg.jpeg)

## _Introduction to Machine Learning Nptel Week 4 Answers_ (Jan-Apr 2025)

**Course Link: [**Click Here**](https://onlinecourses.nptel.ac.in/noc25_cs46/preview)**

***

**Q1.** The Perceptron Learning Algorithm can always converge to a solution if the dataset is linearly separable.

a) True\
b) False\
c) Depends on learning rate\
d) Depends on initial weights

[View Answer](https://my.progiez.com/courses/introduction-to-machine-learning-answers/)

***

**Q2.** Consider the 1-dimensional dataset:

State true or false: The dataset becomes linearly separable after using basis expansion with the following basis function ϕ(x) = \[1 x²]

a) True\
b) False

[View Answer](https://my.progiez.com/courses/introduction-to-machine-learning-answers/)

***

**Q3.** For a binary classification problem with the hinge loss function max(0,1−y(w⋅x)), which of the following statements is correct?

a) The loss is zero only when the prediction is exactly equal to the true label\
b) The loss is zero when the prediction is correct and the margin is at least 1\
c) The loss is always positive\
d) The loss increases linearly with the distance from the decision boundary regardless of classification

[View Answer](https://my.progiez.com/courses/introduction-to-machine-learning-answers/)

***

**Q4.** For a dataset with n points in d dimensions, what is the maximum number of support vectors possible in a hard-margin SVM?

a) 2\
b) d\
c) n/2\
d) n

[View Answer](https://my.progiez.com/courses/introduction-to-machine-learning-answers/)

***

**Q5.** In the context of soft-margin SVM, what happens to the number of support vectors as the parameter C increases?

a) Generally increases\
b) Generally decreases\
c) Remains constant\
d) Changes unpredictably

[View Answer](https://my.progiez.com/courses/introduction-to-machine-learning-answers/)

***

**Q6.** Which of these is not a support vector when using a Support Vector Classifier with a polynomial kernel (degree = 3, C = 1, gamma = 0.1)?

a) 2\
b) 1\
c) 9\
d) 10

[View Answer](https://my.progiez.com/courses/introduction-to-machine-learning-answers/)

***

**Q7.** Train a Linear perceptron classifier on the modified Iris dataset. Use only the first two features for your model and report the best classification accuracy for L1 and L2 penalty terms.

a) 0.91, 0.64\
b) 0.88, 0.71\
c) 0.71, 0.65\
d) 0.78, 0.64

[View Answer](https://my.progiez.com/courses/introduction-to-machine-learning-answers/)

***

**Q8.** Train an SVM classifier on the modified Iris dataset. Use only the first three features. Try different hyperparameters with RBF kernel, gamma = 0.5, one-vs-rest classifier, no feature normalization. Try C = 0.01, 1, 10. Report the best classification accuracy.

a) 0.98\
b) 0.88\
c) 0.99\
d) 0.92

[View Answer](https://my.progiez.com/courses/introduction-to-machine-learning-answers/)

***


## _Introduction to Machine Learning Nptel Week 4 Answers_ (July-Dec 2024)

**Course Link: [**Click Here**](https://onlinecourses.nptel.ac.in/noc24_cs101/)**

***

**1. In the context of the perceptron learning algorithm, what does the expression f(x)||f'(x)|| represent?**\
a) The gradient of the hyperplane\
b) The signed distance to the hyperplane\
c) The normal vector to the hyperplane\
d) The misclassification error

**Answer:** **b) The signed distance to the hyperplane**

***

**2. Why do we normalize by ∥β∥ (the magnitude of the weight vector) in the SVM objective function?**\
a) To ensure the margin is independent of the scale of β\
b) To minimize the computational complexity of the algorithm\
c) To prevent overfitting\
d) To ensure the bias term is always positive

**Answer: a) To ensure the margin is independent of the scale of β**

***

**3. Which of the following is NOT one of the KKT conditions for optimization problems with inequality constraints?**\
a) Stationarity: ∇f(x∗)+∑mi=1λi∇gi(x∗)+∑pj=1νj∇hj(x∗)=0\
b) Primal feasibility: gi(x∗)≤0 for all i, and hj(x∗)=0 for all j\
c) Dual feasibility: λi≥0 for all i\
d) Convexity: The objective function f(x) must be convex

**Answer:d) Convexity: The objective function f(x) must be convex**

***

**4. Consider the 1-dimensional dataset: (Note: x is the feature and y is the output) State true or false: The dataset becomes linearly separable after using basis expansion with the following basis function ϕ(x)=\[1 x³]**\
a) True\
b) False

**Answer:a) True**

***

**5. Consider a polynomial kernel of degree d operating on p-dimensional input vectors. What is the dimension of the feature space induced by this kernel?**\
a) p×d\
b) (p+1)×d\
c) (p+dd)\
d) pd

**Answer c ) (p+dd)**

[****See also**  **NPTEL Introduction To Machine Learning IITKGP ASSIGNMENT 7****](https://progiez.com/nptel-introduction-to-machine-learning-iitkgp-assignment-7)

***

**6. State True or False: For any given linearly separable data, for any initialization, both SVM and Perceptron will converge to the same solution**\
a) True\
b) False

**Answer: b) False**

***

**7. Train a Linear perceptron classifier on the modified iris dataset. We recommend using sklearn. Use only the first two features for your model and report the best classification accuracy for l1 and l2 penalty terms.**\
a) 0.91, 0.64\
b) 0.88, 0.71\
c) 0.71, 0.65\
d) 0.78, 0.64

**Answer:b) 0.88, 0.71**

***

**8. Train a SVM classifier on the modified iris dataset. We recommend using sklearn. Use only the first three features. We encourage you to explore the impact of varying different hyperparameters of the model. Specifically try different kernels and the associated hyperparameters. As part of the assignment train models with the following set of hyperparameters RBF-kernel, gamma=0.5, one-vs-rest classifier, no-feature-normalization. Try C=0.01, 1, 10. For the above set of hyperparameters, report the best classification accuracy.**\
a) 0.98\
b) 0.88\
c) 0.99\
d) 0.92

**Answer: a) 0.98**

***

All weeks of Introduction to Machine Learning: [Click Here](https://progiez.com/nptel-assignment-answers/introduction-to-internet-of-things)

For answers to additional Nptel courses, please refer to this link: [NPTEL Assignment Answers](https://progiez.com/nptel-assignment-answers)

***


## Introduction to Machine Learning Nptel Week 4 Answers (Jan-Apr 2024)

**Course name: Introduction to Machine Learning**

**Course Link: [**Click Here**](https://onlinecourses.nptel.ac.in/noc24_cs51/preview)**

**_**For answers or latest updates join our telegram channel: [**Click here to join**](https://telegram.me/nptel_assignments)**_**

***

Q1. For a two-class classification problem, we use an SVM classifier and obtain the following separating hyperplane. We have marked 4 instances of the training data. Identify the point which will have the most impact on the shape of the boundary on its removal.\
1\
2\
3\
4

**Answer: 1**

***

**Q2. Consider a soft-margin SVM with a linear kernel and no slack variables, trained on n points. The number of support vectors returned is k. By adding one extra point to the dataset and retraining the classifier, what is the maximum possible number of support vectors that can be returned (tuning parameter C)?**\
k\
n\
n + 1\
k + 1

**Answer: n + 1**

***

**_**For answers or latest updates join our telegram channel: [**Click here to join**](https://telegram.me/nptel_assignments)**_**

**_****These are Introduction to Machine Learning Nptel Week 4 Answers****_**

***

**Q3. Consider the data set given below.\
Claim: The PLA (Perceptron Learning Algorithm) can learn a classifier that achieves zero misclassification error on the training data. This claim is:**\
True\
False\
Depends on the initial weights\
True, only if we normalize the feature vectors before applying PLA.

**Answer: False**

***

**Q4. Consider the following dataset:\
(Note: x is the feature and y is the output)\
Which of these is not a support vector when using a Support Vector Classifier with a polynomial kernel with degree = 3, C = 1, and gamma = 0.1?\
(We recommend using sklearn to solve this question.)**\
3\
1\
9\
10

**Answer: 3**

***

**_**For answers or latest updates join our telegram channel: [**Click here to join**](https://telegram.me/nptel_assignments)**_**

**_****These are Introduction to Machine Learning Nptel Week 4 Answers****_**

***

**Q5. Which of the following is/are true about the Perceptron classifier?**\
It can learn a OR function\
It can learn a AND function\
The obtained separating hyperplane depends on the order in which the points are presented in the training process.\
For a linearly separable problem, there exists some initialization of the weights which might lead to non-convergent cases.

**Answer: a, b, c**

***

**Q6. In SVMs, a large functional margin represents a confident and correct prediction. Let the functional margins be defined as :\
y^(i)=y(i)(wTx+b)\
and the linear classifier as hw,b(x)=g(wTx+b) For any choice of suitable g(·), if we replace w by 2w and b by 2b, which of the following is likely to be observed?**\
No change in hw,b(x)=g(wTx+b).\
Will result in reducing the functional margin by half.\
Change in geometric margin.\
None of the above.

**Answer: No change in hw,b(x)=g(wTx+b).**

***

**_**For answers or latest updates join our telegram channel: [**Click here to join**](https://telegram.me/nptel_assignments)**_**

**_****These are Introduction to Machine Learning Nptel Week 4 Answers****_**

***

**Q7. Consider the following optimization problem\
min x2+1\
s.t. (x−2)(x−4)≤0. Select the correct options regarding this optimization problem.**\
Strong Duality holds.\
Strong duality doesn’t hold.\
The Lagrangian can be written as L(x,λ)=(1+λ)x2−6λx+1+8λ\
The dual objective will be g(λ)=−9λ21+λ+1+8λ

**Answer: a, c**

***

**Q8. Suppose you have trained an SVM which is not performing well, and hence you have constructed more features from existing features for the model. Which of the following statements may be true?**\
We are lowering the bias.\
We are lowering the variance.\
We are increasing the bias.\
We are increasing the variance.

[****See also**  **NPTEL Introduction To Machine Learning IITKGP ASSIGNMENT 9****](https://progiez.com/nptel-introduction-to-machine-learning-iitkgp-assignment-9)

**Answer: a, d**

***

**_**For answers or latest updates join our telegram channel: [**Click here to join**](https://telegram.me/nptel_assignments)**_**

More Weeks of Introduction to Machine Learning: [Click here](https://progiez.com/nptel-assignment-answers/introduction-to-machine-learning/)

More Nptel Courses: <https://progiez.com/nptel-assignment-answers>

***


## Introduction to Machine Learning Nptel Week 4 Answers (July-Dec 2023)

**Course Name: Introduction to Machine Learning**

**Course Link: [**Click Here**](https://onlinecourses.nptel.ac.in/noc23_cs98/course)**

**_****These are Introduction to Machine Learning Nptel Week 4 Answers****_**

***

**Q1. Consider the data set given below.\
Claim: PLA (perceptron learning algorithm) can learn a classifier that achieves zero misclassification error on the training data. This claim is:**\
True\
False\
Depends on the initial weights\
True, only if we normalize the feature vectors before applying PLA.

**Answer: False**

***

**Q2. Which of the following loss functions are convex? (Multiple options may be correct)**\
0-1 loss (sometimes referred as mis-classification loss)\
Hinge loss\
Logistic loss\
Squared error loss

**Answer: b, c, d**

***

**_****These are Introduction to Machine Learning Nptel Week 4 Answers****_**

***

**Q3. Which of the following are valid kernel functions?**\
(1+)d\
tanh(K1+K2)\
exp(−γ||x−x’||2)

**Answer: a, b, c**

***

**Q4. Consider the 1 dimensional dataset:\
(Note: x is the feature, and y is the output)\
State true or false: The dataset becomes linearly separable after using basis expansion with the following basis function ϕ(x)=\[1×3]**\
True\
False

**Answer: False**

***

**_****These are Introduction to Machine Learning Nptel Week 4 Answers****_**

***

**Q5. State True or False:\
SVM cannot classify data that is not linearly separable even if we transform it to a higherdimensional space.**\
True\
False

**Answer: False**

***

**_****These are Introduction to Machine Learning Nptel Week 4 Answers****_**

***

**Q6. State True or False:\
The decision boundary obtained using the perceptron algorithm does not depend on the initial values of the weights.**\
True\
False

**Answer: False**

***

**_****These are Introduction to Machine Learning Nptel Week 4 Answers****_**

***

**Q7. Consider a linear SVM trained with n labeled points in R2 without slack penalties and resulting in k=2 support vectors, where n>100. By removing one labeled training point and retraining the SVM classifier, what is the maximum possible number of support vectors in the resulting solution?**\
1\
2\
3\
n − 1\
n

**Answer: n − 1**

***

**_****These are Introduction to Machine Learning Nptel Week 4 Answers****_**

***

**Q8. Consider an SVM with a second order polynomial kernel. Kernel 1 maps each input data point x to K1(x)=\[xx2]. Kernel 2 maps each input data point x to K2(x)=\[3x3x2]. Assume the hyper-parameters are fixed. Which of the following option is true?**\
The margin obtained using K2(x) will be larger than the margin obtained using K1(x).\
The margin obtained using K2(x) will be smaller than the margin obtained using K1(x).\
The margin obtained using K2(x) will be the same as the margin obtained using K1(x).

**Answer: The margin obtained using K2(x) will be larger than the margin obtained using K1(x).**


Introduction To Machine Learning IIT-KGP Week 4 Answers (July-Dec 2024)


# Introduction To Machine Learning IIT-KGP Week 4 Answers (July-Dec 2024)

**Q1.**A man is known to speak the truth 2 out of 3 times. He throws a die and reports that the number obtained is 4. Find the probability that the number obtained is actually 4:\
Α. 2/3\
Β. 3/4\
C. 5/22\
D. 2/7

**Answer:** [**Click here to view Answers**](https://progiez.com/introduction-to-machine-learning-iit-kgp-week-4-answers)

**Q2.**Two cards are drawn at random from a deck of 52 cards without replacement. What is the probability of drawing a 2 and an Ace in that order?\
Α. 4/51\
Β. 1/13\
C. 4/256\
D. 4/663

**Answer:** [**Click here to view Answers**](https://progiez.com/introduction-to-machine-learning-iit-kgp-week-4-answers)

**For answers or latest updates join our telegram channel:** [**Click here to join**](https://telegram.me/nptel_assignments)

**These are Introduction To Machine Learning IIT-KGP Week 4 Answers**

**Q3**.Consider the following graphical model, mark which of the following pair of random variables are independent given no evidence?\
A. a,b\
B. c,d\
C. e,d\
D. c,e

**Answer:** [**Click here to view Answers**](https://progiez.com/introduction-to-machine-learning-iit-kgp-week-4-answers)

**Q4**.Consider the following Bayesian network. The random variables given in the model are modeled as discrete variables (Rain = R, Sprinkler = S and Wet Grass = W) and the corresponding probability values are given below. (Note: (X) represents complement of X)

P(R) = 0.1\
P(S) = 0.2\
P(W | R, S) = 0.8\
P(W | R,S) = 0.7\
P(WR, S) = 0.6\
P(W-R,S) = 0.5\
Calculate P(S | W, R).\
A. 1\
B. 0.5\
C. 0.22\
D. 0.78

**Answer:** [**Click here to view Answers**](https://progiez.com/introduction-to-machine-learning-iit-kgp-week-4-answers)

**For answers or latest updates join our telegram channel:** [**Click here to join**](https://telegram.me/nptel_assignments)

**These are Introduction To Machine Learning IIT-KGP Week 4 Answers**

**Q5.**What is the naive assumption in a Naive Bayes Classifier?\
A. All the classes are independent of each other\
B. All the features of a class are independent of each other\
C. The most probable feature for a class is the most important feature to be considered for classification\
D. All the features of a class are conditionally dependent on each other.

**Answer:** [**Click here to view Answers**](https://progiez.com/introduction-to-machine-learning-iit-kgp-week-4-answers)

**Q6** .A drug test (random variable T) has 1% false positives (i.e., 1% of those not taking drugs show positive in the test), and 5% false negatives (i.e., 5% of those taking drugs test negative). Suppose that 2% of those tested are taking drugs. Determine the probability that somebody who tests positive is actually taking drugs (random variable D).\
A. 0.66\
Β. 0.34\
C. 0.50\
D. 0.91

**Answer:** [**Click here to view Answers**](https://progiez.com/introduction-to-machine-learning-iit-kgp-week-4-answers)

**For answers or latest updates join our telegram channel:** [**Click here to join**](https://telegram.me/nptel_assignments)

**These are Introduction To Machine Learning IIT-KGP Week 4 Answers**

Q7.It is given that P(A/B) = 2/3 and P(A/B) = 1/4. Compute the value of P(B|A).\
A. 1/2\
B. 3/3\
C. 3/4\
D. Not enough information.

**Answer:** [**Click here to view Answers**](https://progiez.com/introduction-to-machine-learning-iit-kgp-week-4-answers)

**Q8.**Consider the following Bayesian network, where F = having the flu and C = coughing:

Find P(C) and P(FIC).\
Α. 0.35, 0.23\
Β. 0.35,0.77\
C. 0.24, 0.024\
D. 0.5, 0.23

**Answer:** [**Click here to view Answers**](https://progiez.com/introduction-to-machine-learning-iit-kgp-week-4-answers)

**For answers or latest updates join our telegram channel:** [**Click here to join**](https://telegram.me/nptel_assignments)

**These are Introduction To Machine Learning IIT-KGP Week 4 Answers**

**Q9.**Bag I contains 4 white and 6 black balls while another Bag II contains 4 white and 3 black balls. One ball is drawn at random from one of the bags and it is found to be black. Find the probability that it was drawn from Bag I.\
A. 1/2\
Β. 2/3\
C. 7/12\
D. 9/23

**Answer:** [**Click here to view Answers**](https://progiez.com/introduction-to-machine-learning-iit-kgp-week-4-answers)

**For answers or latest updates join our telegram channel:** [**Click here to join**](https://telegram.me/nptel_assignments)

**These are Introduction To Machine Learning IIT-KGP Week 4 Answers**

**Q10**.In a Bayesian network a node with only outgoing edge(s) represents\
A. a variable conditionally independent of the other variables.\
B. a variable dependent on its siblings.\
C. a variable whose dependency is uncertain.\
D. None of the above.

**Answer:** [**Click here to view Answers**](https://progiez.com/introduction-to-machine-learning-iit-kgp-week-4-answers)

**For answers or latest updates join our telegram channel:** [**Click here to join**](https://telegram.me/nptel_assignments)

**These are Introduction To Machine Learning IIT-KGP Week 4 Answers**

All weeks of Introduction to Machine Learning: [Click Here](https://progiez.com/nptel-assignment-answers/introduction-to-machine-learning)

More Nptel Courses: <https://progiez.com/nptel-assignment-answers>
